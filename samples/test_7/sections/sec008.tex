%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                 %
%                     SECTION                     %
%                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Metrics}
\label{sec:sec008}

Our user test metrics refers to user performance measured against specific performance goals necessary to satisfy the test requirements. Scenario completion success rates, adherence to dialog scripts, error rates, and subjective evaluations will be used. Time-to-Completion (TtC)~\cite{ioannidis1998effect} of scenarios will also be collected. From the set of tasks (Section \ref{sec:sec007}), each task corresponds to the set of {\it Phases}, {\it Scenarios} and {\it Activities} (Section \ref{sec:sec001}), meaning that we first need to explain it relations.

The first two tasks, {\it i.e.}, {\bf Task 1.1.1} and {\bf Task 1.1.2}, are related to the {\bf Pha1.} phase, as well as with {\bf Act1.}, {\bf Act2.} and {\bf Act3.} activities. The {\bf Pha2.} phase focus on testing and analyzing the {\it \gls{MM}} condition {\it \gls{MM}}, {\it i.e.}, corresponding to {\bf Sce1.} scenario. The six next tasks, {\it i.e.}, {\bf Task 2.1.1}, {\bf Task 2.1.2}, {\bf Task 2.1.3}, {\bf Task 2.2.1}, {\bf Task 2.2.2} and {\bf Task 2.2.3}, are related to {\bf Pha2.} phase. Also, the {\bf Pha2.} phase is related with {\bf Act4.}, {\bf Act5.} and {\bf Act6.} activities, of the {\bf Sce1.} scenario, for diagnosing {\bf Pat1.}, {\bf Pat2.} and {\bf Pat3.} patients. The second next two tasks, {\it i.e.}, {\bf Task 2.3.1} and {\bf Task 2.3.2}, are related with {\bf Act7.} activity of {\bf Pha2.} phase. Now, on the {\bf Pha3.} phase, we will have a relation between testing and analyzing the {\it Assistant (Assis.)} condition, {\it i.e.}, corresponding to {\bf Sce2.} scenario. The six next tasks, {\it i.e.}, {\bf Task 3.1.1}, {\bf Task 3.1.2}, {\bf Task 3.1.3}, {\bf Task 3.2.1}, {\bf Task 3.2.2} and {\bf Task 3.2.3}, are therefore related to {\bf Pha3.} phase. Also, the {\bf Pha3.} phase is related with {\bf Act4.}, {\bf Act5.}, {\bf Act6.} and {\bf Act8.} activities, but this time of the {\bf Sce2.} scenario, for diagnosing {\bf Pat1.}, {\bf Pat2.} and {\bf Pat3.} patients. At the end, the last two tasks, {\it i.e.}, {\bf Task 3.3.1} and {\bf Task 3.3.2}, are related with {\bf Act7.} activity for {\bf Pha3.} phase.

\subsection{Patient Classification}

For the patient classification, we will use the well known scale for classifying the breast cancer disease called \hyperlink{https://en.wikipedia.org/wiki/BI-RADS}{BIRADS}~\cite{balleyguier2007birads}. The \hyperlink{https://en.wikipedia.org/wiki/BI-RADS}{BIRADS} scale is a scheme for putting the findings from breast into a small number of well-defined categories~\cite{obenauer2005applications}.

\hfil

The BIRADS assessment categories are:

\begin{itemize}
\item 0 - Incomplete;
\item 1 - Negative;
\item 2 - Benign Findings;
\item 3 - Probably Benign;
\item 4 - Suspicious Abnormality;
\item 5 - Highly Suspicious of Malignancy;
\item 6 - Known Biopsy Proven Malignancy;
\end{itemize}

For each participant, we will ask the respective examination and respective BIRADS value. From here, we will register the respective value per each scenario, both {\it i.e.}, {\bf Sce1.} and {\bf Sce2.} scenarios. At the end, we can compare the values provided between {\bf Sce1.} and {\bf Sce2.} scenarios. On {\bf Sce1.} scenario, it is where we just improve the visualization technique. Now, with {\bf Sce2.} scenario, {\it i.e.}, an {\it AI-Assistance} diagnosis, we want to understand if the given severity value changed and improved. Also, on {\bf Sce2.} scenario, we want to understand where, {\it i.e.}, on {\it Assistant} or {\it Heatmap} prototype, did the participant took the final improved answer. Nevertheless, we will also compare several other patients' variables, like pathology, to address several other clinical issues.

\subsection{Workload}

To measure the workload, we used the \hyperlink{https://en.wikipedia.org/wiki/NASA-TLX}{NASA Task Load Index (NASA-TLX)}~\cite{ramkumar2017using} scale. The scale is a subjective workload assessment tool that will allow us to perform subjective workload assessments on our participants. For the purpose, we created a repository~\cite{https://doi.org/10.13140/rg.2.2.25301.06883, francisco_maria_calisto_2018_1435044} to cover this need of content.

\hfill

By incorporating a multi-dimensional rating procedure, NASA-TLX derives an overall workload score based on a weighted average of ratings on six sub-scales:

\begin{itemize}
\item Mental Demand
\item Physical Demand
\item Temporal Demand
\item Performance
\item Effort
\item Frustration 
\end{itemize}

At the end, each participant will provide answers regarding the workload information during {\bf Act5.} activity, of {\bf Sce1.} and {\bf Sce2.} scenarios, on both {\bf Pha1.} and {\bf Pha2.} phases respectively. This will also cover both {\bf Task 2.3.1} and {\bf Task 3.3.1} tasks.

\subsection{Usability}

To measure the usability, we used the \hyperlink{https://en.wikipedia.org/wiki/System_usability_scale}{System Usability Scale (SUS)}~\cite{orfanou2015perceived}. The \hyperlink{https://en.wikipedia.org/wiki/System_usability_scale}{SUS} provides a ``quick and dirty", reliable tool for measuring the usability. It consists of a 10 item questionnaire with ten response options for respondents; from {\it Strongly Agree} to {\it Strongly Disagree}. Originally created by John Brooke in 1986, it allows you to evaluate a wide variety of products and services, including hardware, software, mobile devices, websites and applications. For the purpose, we created a repository~\cite{https://doi.org/10.13140/rg.2.2.26978.79044, francisco_maria_calisto_2018_1435042} to cover this need of content.

\hfill

When using \hyperlink{https://en.wikipedia.org/wiki/System_usability_scale}{SUS}, participants are asked to score the following 10 items with one of ten responses that range from {\bf Strongly Agree} to {\bf Strongly Disagree}:

\begin{enumerate}
\item I think that I would like to use this system frequently.
\item I found the system unnecessarily complex.
\item I thought the system was easy to use.
\item I think that I would need the support of a technical person to be able to use this system.
\item I found the various functions in this system were well integrated.
\item I thought there was too much inconsistency in this system.
\item I would imagine that most people would learn to use this system very quickly.
\item I found the system very cumbersome to use.
\item I felt very confident using the system.
\item I needed to learn a lot of things before I could get going with this system.
\end{enumerate}

Again, each participant will provide answers regarding the system's usability information during {\bf Act5.} activity, of {\bf Sce1.} and {\bf Sce2.} scenarios, from both {\bf Pha1.} and {\bf Pha2.} phases respectively. This will also cover both {\bf Task 2.3.1} and {\bf Task 3.3.1} tasks.

\subsection{Trust}

The \gls{DOTS}~\cite{francisco_maria_calisto_2019_2671717, https://doi.org/10.13140/rg.2.2.23078.37448} was introduced on a recent work~\cite{Cai:2019:EEE:3301275.3302289, Cai:2019:HTC:3290605.3300234}, introducing the concept of measuring \underline{trust} across {\it AI} systems.
For that, we created a repository~\cite{francisco_maria_calisto_2019_2671717} supporting our user tests.
\gls{DOTS} is a scale to measure the trustworthiness of our {\it AI-Assisted} system.
Therefore, we created a three items list of questions on a 20-point scale of Likert-style~\cite{joshi2015likert}.
The following list, represents the questions adapted from this model~\cite{mayer1995integrative} addressing each of the three items, {\it i.e.}, {\it understanding}, {\it capability} and {\it benevolence}~\cite{Cai:2019:EEE:3301275.3302289}.

\begin{enumerate}
\item I understand what the system is thinking. ({\bf Understanding})
\item The system seems capable. ({\bf Capability})
\item The system seems benevolent. ({\bf Benevolence})
\end{enumerate}

Each participant will provide answers regarding the system's trustworthiness during {\bf Act5.} activity, of {\bf Sce2.} scenario, from {\bf Pha2.} phase.
This will cover the {\bf Task 3.3.1} from the list of tasks.

\subsection{Predictions}

To measure system predictions with purpose of comparing participants acceptance, we applied our own computational method.
The computational method is as follows, while we defined several variables to it, defined next to this information and further explained.
Let the {\it Overall Accuracy}~\cite{ashraf2018comparative, li2018digital} be $\O$, a variable following the discrete uniform distribution as $\O \in \mathbb{R}$. The accuracy is used by us to measure how accurate is the overall performance of our solution, considering both positive and negative classes without worrying about data imbalance. Let {\it Total Number of Correct Predictions}~\cite{ashraf2018comparative, li2018digital} be $\tau$, a variable following the discrete uniform distribution as $\tau \in \mathbb{R}$.
Let {\it All Possible Predictions}~\cite{ashraf2018comparative, li2018digital} be $\alpha$, a variable following the discrete uniform distribution as $\alpha \in \mathbb{R}$. As follows, we report our computational method.

\hfill

Computational method to measure the {\it Overall Accuracy} of our solution:

\begin{Form}
\large
\begin{center}
$Overall~Accuracy$ = $\frac{Total~Number~of~Correct~Predictions}{All~Possible~Predictions}$
\end{center}
\end{Form}

\subsection{Eye Tracking}

Eye movement data was collected for several groups of subjects recruited from the same Portuguese institutions, both public and private, with breast domain-expertise levels, while participants inspected (Section \ref{sec:sec007}) breast images. Medical images will be presented to participants on a monitor (Section \ref{sec:sec005}) attached to a 90Hz eye tracking device, called \hyperlink{https://gaming.tobii.com/product/tobii-eye-tracker-4c/}{Tobii Eye Tracker 4C} with reported accuracy for the collection of eye movement data.

For the eye tracking measurements, we will use the work done by Vaidyana-than et al.~\cite{vaidyanathan2014recurrence}, titled as "{\it Recurrence Quantification Analysis Reveals Eye-Movement Behavior Differences between Experts and Novices}". The work uses eye movement data from medical experts and novices, while they inspected several medical images. Most importantly, the work describe and demonstrate how Recurrence Quantification Analysis (RQA)~\cite{anderson2013recurrence}, and the associated measures, can be used to differentiate eye movement behavior during different viewing conditions and image type finding significant differences. From this work, we aim to use their RQA method to measure and quantify certain eye movement aspects, defined as: (1) Recurrence (REC); (2) Determinism (DET); (3) Laminarity (LAM); and (4) Center Of Recurrence Mass (CORM). We are not yet sure if will use all the presented information, however, the hereby {\it User Testing Guide} serves the purpose of presenting all options for the tests. At this point, we are concern with addressing and collecting the maximum user data as possible.

\subsection{Qualitative Evaluation}

Qualitative and subjective evaluations regarding ease of use and satisfaction will be collected. This collection will be done via {\it \hyperlink{https://www.nngroup.com/articles/open-ended-questions/}{open-ended questions}}~\cite{abelson2016supporting, merchant2018digital}, and during debriefing at the conclusion of the session.

The {\it open-ended questions} will utilize free-form responses and feedback, when possible. Whenever possible, it's best to ask {\it \hyperlink{https://www.nngroup.com/articles/open-ended-questions/}{open-ended questions}} so we can find out more than we can anticipate. We will test our questions by trying to answer them with short answers, and rewrite those to find out more about {\it how} and {\it what}. In some cases, we won't be able to accommodate free-form or write-in answers, though, and then it is necessary to limit the possibilities.

\subsection{Scenario Completion}

Each scenario, {\it i.e.}, {\bf Sce1.} and {\bf Sce2.} scenarios, will require, or request, that the participant obtains, or inputs, specific data. This data would be used in course of a typical task. The scenario is completed when the participant indicates the scenario's goal has been obtained. Whether successfully or unsuccessfully. Or the scenario is completed when the participant requests and receives sufficient guidance as to warrant scoring the scenario as a critical error.

\subsection{Time Completion}

The time to complete (ToT)~\cite{delgado2017time, huang2018impact} each scenario, {\it i.e.}, {\bf Sce1.} and {\bf Sce2.} scenarios, not including qualitative and subjective evaluation durations, will be recorded. From this measure, it will be also possible to collect more specific metrics, such as the percentage of time that participants follow an optimal path or the number of times participants need to backtrack.

\subsection{Critical Errors}

Critical Errors are deviations at completion from the targets of the scenario. Obtaining or otherwise reporting of the wrong data value due to participant workflow is a Critical Error. Participants may or may not be aware that the task goal is incorrect or incomplete.

An example of a Critical Error, could be a situation where the participant is not able to open a patient. From this error, we can not even proceed to the next tasks and complete the user test. Despite of the independent completion of the scenario is the goal, we need to guarantee the execution of the test, however, when this errors occur, the facilitator must act.

Critical Errors can also be assigned when the participant initiates, or attempts to initiate, an action that will result in the goal state becoming unobtainable. In general, Critical Errors are unresolved errors preventing completion of the task or errors that produce an incorrect outcome.

\subsection{Non-Critical Errors}

Non-Critical Errors, are errors that are recovered from and by the participant. Or, if not detected, do not result in processing problems or unexpected results. Although Non-Critical Errors can be undetected by the participant, when they are detected they are generally frustrating to the participant.

These errors may be procedural, in which the participant does not complete a scenario in the most optimal means ({\it e.g.}, excessive steps and keystrokes). These errors may also be errors of confusion ({\it e.g.}, initially selecting the wrong function, using a UI control incorrectly such as attempting to edit an un-editable field).

Non-Critical Errors can always be recovered from during the process of completing the scenario. Exploratory behavior, such as opening the wrong menu while searching for a function, will be coded as a non-critical error.